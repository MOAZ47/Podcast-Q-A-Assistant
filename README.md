# ğŸ™ï¸ Podcast Q&A Assistant with Multimodal RAG + Agentic AI Framework

A Multimodal LLM Agentic AI application that allows users to upload podcast/audio files, transcribes them using Whisper, summarizes the content, fact-checks the claims using an LLM + internet search, and delivers a clear, confidence-rated report â€” all through a modular multi-agent pipeline.

---

## ğŸš€ Features

âœ… Upload and analyze any podcast/audio file  
âœ… Automatic transcription using OpenAI Whisper  
âœ… Summary generation via LLM  
âœ… Claim-level fact-checking using external evidence  
âœ… Agent-based architecture: transcriber, summarizer, fact-checker, reporter  
âœ… Interactive Streamlit UI  
âœ… Few-shot prompting with example control  
âœ… Session memory, rerun/reset support  
âœ… Modular codebase for easy scaling

---

## ğŸ“¦ Tech Stack

| Component       | Technology                                |
|------------------|--------------------------------------------|
| Transcription    | [OpenAI Whisper](https://github.com/openai/whisper) |
| LLM              | [Cohere LLM](https://cohere.com) via LangChain |
| Fact Checking    | LLM + parallel async web search (Tavily API)   |
| Vector Store     | [Weaviate](https://weaviate.io/) (optional) |
| Prompting        | LangChain's `FewShotPromptTemplate`        |
| Frontend         | Streamlit                                  |
| Embeddings       | Sentence Transformers via HuggingFace      |

---

## ğŸ§  Architecture Overview

```plaintext
User Uploads Audio
        â”‚
        â–¼
[ Transcriber Agent ]  â† Whisper
        â”‚
        â–¼
[ Summarizer Agent ]   â† LLM (Cohere or others)
        â”‚
        â–¼
[ Fact-Checker Agent ] â† LLM + Internet
        â”‚
        â–¼
[ Reporter Agent ]     â† Few-shot LLM Report Generation
        â”‚
        â–¼
Streamlit UI â†’ Transcript + Final Report

```
---

| Stage              | Before          | After Optimization | Improvement                                     |
| ------------------ | --------------- | ------------------ | ----------------------------------------------- |
| **Transcription**  | \~83 seconds    | \~68 seconds       | âœ… Faster model loading, CPU prewarming          |
| **Summarization**  | \~17 seconds    | \~2â€“7 seconds      | âœ… Switched to Cohere + chunked async processing |
| **Fact Check**     | \~14â€“18 seconds | \~6â€“11 seconds     | âœ… Parallel async search using `threading`       |
| **Report Gen**     | \~95 seconds ğŸ˜… | \~2â€“3 seconds ğŸ˜   | âœ… Optimized FewShot prompts and LLM chaining    |
| **Total Pipeline** | \~160 seconds   | \~100â€“130 seconds    | ğŸš€ **30% reduction in latency**              |



---
## ğŸ§© How It Works
<ul>
        <li>Upload an audio file.</li>
        <li>transcription.py uses Whisper to convert it into text.</li>
        <li>cohere_summarizer.py breaks large transcripts into chunks and summarizes them via Cohereâ€™s LLM.</li>
        <li>factchecker.py extracts factual claims and verifies them using an LLM agent backed by parallel web search (Tavily).</li>
        <li>reporter.py uses few-shot prompts to generate a clean, confidence-rated markdown report.</li>
</ul>

---

## Installation
```bash
git clone https://github.com/Moaz47/Podcast-Q-A-Assistant.git
cd agentic-audio-rag

# Optional: set up a virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```
---

## Running the code
```bash
streamlit run app.py

```

---

## ğŸ“ Project Structure
```plaintext
.
â”œâ”€â”€ app.py               # Streamlit app UI
â”œâ”€â”€ main.py              # Orchestrates the pipeline
â”œâ”€â”€ config.py            # API keys
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ transcription.py
â”‚   â”œâ”€â”€ cohere_summarizer.py
â”‚   â”œâ”€â”€ factchecker.py
â”‚   â””â”€â”€ reporter.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```
---

## âœ¨ Future Improvements
<ul>
    <li>Add chat interface for user questions over the podcast. </li>
    <li>Exportable PDF reports. </li>
    <li>Multi-turn memory for user corrections. </li>
    <li>OpenAI / Gemini model switching. </li>
</ul>